{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:52:59.437619Z",
     "start_time": "2021-01-07T10:52:59.434468Z"
    }
   },
   "source": [
    "# 依赖和配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:41:30.094259Z",
     "start_time": "2021-01-07T10:41:30.084577Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "max_train_epochs = 5\n",
    "warmup_proportion = 0.05\n",
    "gradient_accumulation_steps = 1\n",
    "train_batch_size = 32\n",
    "valid_batch_size = train_batch_size\n",
    "test_batch_size = train_batch_size\n",
    "data_workers= 2\n",
    "# save_checkpoint = False\n",
    "\n",
    "learning_rate=2e-5\n",
    "weight_decay=0.01\n",
    "max_grad_norm=1.0\n",
    "\n",
    "# predict_max_label_len = 10\n",
    "    \n",
    "cur_time = time.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "base_path = 'neural-chinese-address-parsing/data/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:42:07.210348Z",
     "start_time": "2021-01-07T10:42:06.346755Z"
    },
    "code_folding": [
     11
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert 102 100 119\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertTokenizer, BertModel, BertForTokenClassification\n",
    "cls_token='[CLS]'\n",
    "eos_token='[SEP]'\n",
    "unk_token='[UNK]'\n",
    "pad_token='[PAD]'\n",
    "mask_token='[MASK]'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "config = BertConfig.from_pretrained('bert-base-chinese')\n",
    "TheModel = BertModel\n",
    "ModelForTokenClassification = BertForTokenClassification\n",
    "\n",
    "eos_id = tokenizer.convert_tokens_to_ids([eos_token])[0]\n",
    "unk_id = tokenizer.convert_tokens_to_ids([unk_token])[0]\n",
    "period_id = tokenizer.convert_tokens_to_ids(['.'])[0]\n",
    "print(model_select, eos_id, unk_id, period_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:42:09.858019Z",
     "start_time": "2021-01-07T10:42:09.850195Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = ['B-assist', 'I-assist', 'B-cellno', 'I-cellno', 'B-city', 'I-city', 'B-community', 'I-community', 'B-country', 'I-country', 'B-devZone', 'I-devZone', 'B-district', 'I-district', 'B-floorno', 'I-floorno', 'B-houseno', 'I-houseno', 'B-otherinfo', 'I-otherinfo', 'B-person', 'I-person', 'B-poi', 'I-poi', 'B-prov', 'I-prov', 'B-redundant', 'I-redundant', 'B-road', 'I-road', 'B-roadno', 'I-roadno', 'B-roomno', 'I-roomno', 'B-subRoad', 'I-subRoad', 'B-subRoadno', 'I-subRoadno', 'B-subpoi', 'I-subpoi', 'B-subroad', 'I-subroad', 'B-subroadno', 'I-subroadno', 'B-town', 'I-town']\n",
    "label2id = {}\n",
    "for i, l in enumerate(labels):\n",
    "    label2id[l] = i\n",
    "num_labels = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:52:59.552152Z",
     "start_time": "2021-01-07T10:52:59.440349Z"
    }
   },
   "source": [
    "# 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:42:10.334834Z",
     "start_time": "2021-01-07T10:42:10.323506Z"
    },
    "code_folding": [
     1,
     5
    ]
   },
   "outputs": [],
   "source": [
    "def get_data_list(f):\n",
    "    data_list = []\n",
    "    origin_token, token, label = [], [], []\n",
    "    for l in f:\n",
    "        l = l.strip().split()\n",
    "        if not l:\n",
    "            data_list.append([token, label, origin_token])\n",
    "            origin_token, token, label = [], [], []\n",
    "            continue\n",
    "        for i, tok in enumerate(l[0]):\n",
    "            token.append(tok)\n",
    "            label.append(label2id[l[1]])\n",
    "        origin_token.append(l[0])\n",
    "    assert len(token) == 0\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:42:11.736247Z",
     "start_time": "2021-01-07T10:42:11.308280Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8957 2985 2985\n",
      "max_token_len 76\n"
     ]
    }
   ],
   "source": [
    "f_train = open(base_path + 'train.txt')\n",
    "f_test = open(base_path + 'test.txt')\n",
    "f_dev = open(base_path + 'dev.txt')\n",
    "\n",
    "train_list = get_data_list(f_train)\n",
    "test_list = get_data_list(f_test)\n",
    "dev_list = get_data_list(f_dev)\n",
    "print(len(train_list), len(test_list), len(dev_list))\n",
    "max_token_len = 0\n",
    "for ls in [train_list, test_list, dev_list]:\n",
    "    for l in ls:\n",
    "        max_token_len = max(max_token_len, len(l[0]))\n",
    "print('max_token_len', max_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:42:41.038942Z",
     "start_time": "2021-01-07T10:42:41.029584Z"
    },
    "code_folding": [
     41
    ]
   },
   "outputs": [],
   "source": [
    "class MyDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.examples[index]\n",
    "        sentence = example[0]\n",
    "        #vaild_id = example[1]\n",
    "        label = example[1]\n",
    "        \n",
    "        pad_len = max_token_len - len(sentence)\n",
    "        total_len = len(sentence)+2\n",
    "        \n",
    "        input_token = [cls_token] + sentence + [eos_token] + [pad_token] * pad_len\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_token)\n",
    "        attention_mask = [1] + [1] * len(sentence) + [1] + [0] * pad_len\n",
    "        # vaild_mask = [0] + vaild_id + [0] + [0] * pad_len\n",
    "        # active_mask = [1] * len(label) + [0] * (max_token_len+2-len(label))\n",
    "        label = [-100] + label + [-100] + [-100] * pad_len\n",
    "        assert max_token_len + 2 == len(input_ids) == len(attention_mask) == len(input_token)# == len(vaild_mask)\n",
    "        \n",
    "        return input_ids, attention_mask, total_len, label, index\n",
    "\n",
    "def the_collate_fn(batch):\n",
    "    total_lens = [b[2] for b in batch]\n",
    "    total_len = max(total_lens)\n",
    "    input_ids = torch.LongTensor([b[0] for b in batch])\n",
    "    attention_mask = torch.LongTensor([b[1] for b in batch])\n",
    "    label = torch.LongTensor([b[3] for b in batch])\n",
    "    input_ids = input_ids[:,:total_len]\n",
    "    attention_mask = attention_mask[:,:total_len]\n",
    "    label = label[:,:total_len]\n",
    "\n",
    "    indexs = [b[4] for b in batch]\n",
    "\n",
    "    return input_ids, attention_mask, label, indexs\n",
    "\n",
    "train_dataset = MyDataSet(train_list)\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers=data_workers,\n",
    "    collate_fn=the_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:42:42.163099Z",
     "start_time": "2021-01-07T10:42:42.158644Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = MyDataSet(test_list)\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle = False,\n",
    "    num_workers=data_workers,\n",
    "    collate_fn=the_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:42:43.605518Z",
     "start_time": "2021-01-07T10:42:43.592007Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def eval():\n",
    "    result = []\n",
    "    for step, batch in enumerate(tqdm(test_data_loader)):\n",
    "        input_ids, attention_mask, label = (b.to(device) for b in batch[:-1])\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = F.softmax(logits, dim=-1)\n",
    "        logits = logits.data.cpu()\n",
    "        logit_list = []\n",
    "        sum_len = 0\n",
    "        for m in attention_mask:\n",
    "            l = m.sum().cpu().item()\n",
    "            logit_list.append(logits[sum_len:sum_len+l])\n",
    "            sum_len += l\n",
    "        assert sum_len == len(logits)\n",
    "        for i, l in enumerate(logit_list):\n",
    "            rr = torch.argmax(l, dim=1)\n",
    "            for j, w in enumerate(test_list[batch[-1][i]][0]):\n",
    "                result.append([w, labels[label[i][j+1].cpu().item()],labels[rr[j+1]]])\n",
    "            result.append([])\n",
    "    print(result[:20])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:42:45.364083Z",
     "start_time": "2021-01-07T10:42:45.360656Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def log(msg):\n",
    "#     with open(cur_time + '.log', 'a') as f:\n",
    "#         f.write(time.strftime(\"%Y-%m-%d_%H:%M:%S\") + '\\t' + str(msg) + '\\n')\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:44:38.975300Z",
     "start_time": "2021-01-07T10:44:38.960814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sxw/jupyter_workspace/pretrain_model/bert_base_wwm_ext_chinese/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:44:58.876338Z",
     "start_time": "2021-01-07T10:44:58.863406Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BertForSeqTagging(ModelForTokenClassification):\n",
    "    def __init__(self):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = TheModel.from_pretrained('bert-base-chinese')\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        self.init_weights()\n",
    "            \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        batch_size, max_len, feature_dim = sequence_output.shape\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        active_loss = attention_mask.view(-1) == 1\n",
    "        active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            active_labels = labels.view(-1)[active_loss]\n",
    "            loss = loss_fct(active_logits, active_labels)\n",
    "            return loss\n",
    "        else:\n",
    "            return active_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:45:14.776530Z",
     "start_time": "2021-01-07T10:45:09.697632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warmup steps : 70\n"
     ]
    }
   ],
   "source": [
    "model = BertForSeqTagging()\n",
    "model.to(device)\n",
    "t_total = len(train_data_loader) // gradient_accumulation_steps * max_train_epochs + 1\n",
    "\n",
    "num_warmup_steps = int(warmup_proportion * t_total)\n",
    "log('warmup steps : %d' % num_warmup_steps)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight'] # no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "param_optimizer = list(model.named_parameters())\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay': weight_decay},\n",
    "    {'params':[p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:52:59.431400Z",
     "start_time": "2021-01-07T10:45:36.339418Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [01:18<00:00,  3.55it/s]\n",
      "  0%|          | 0/94 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 Epoch Mean Loss 0.9018 Time 1.32 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:09<00:00, 10.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['龙', 'B-town', 'B-town'], ['港', 'I-town', 'I-town'], ['镇', 'I-town', 'I-town'], ['泰', 'B-poi', 'B-poi'], ['和', 'I-poi', 'I-poi'], ['小', 'I-poi', 'I-poi'], ['区', 'I-poi', 'I-poi'], ['B', 'B-houseno', 'B-houseno'], ['懂', 'I-houseno', 'I-poi'], ['1', 'B-roomno', 'B-roomno'], ['0', 'B-roomno', 'B-roomno'], ['9', 'B-roomno', 'B-roomno'], ['7', 'B-roomno', 'B-roomno'], [], ['浙', 'B-prov', 'B-prov'], ['江', 'I-prov', 'I-prov'], ['省', 'I-prov', 'I-prov'], ['嘉', 'B-city', 'B-city'], ['兴', 'I-city', 'I-city'], ['市', 'I-city', 'I-city']]\n",
      "0.7509987159366529\n",
      "Can't open perl script \"conlleval.pl\": No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [01:19<00:00,  3.54it/s]\n",
      "  0%|          | 0/94 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 Epoch Mean Loss 0.6026 Time 1.32 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:08<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['龙', 'B-town', 'B-town'], ['港', 'I-town', 'I-town'], ['镇', 'I-town', 'I-town'], ['泰', 'B-poi', 'B-poi'], ['和', 'I-poi', 'I-poi'], ['小', 'I-poi', 'I-poi'], ['区', 'I-poi', 'I-poi'], ['B', 'B-houseno', 'B-houseno'], ['懂', 'I-houseno', 'I-subpoi'], ['1', 'B-roomno', 'B-roomno'], ['0', 'B-roomno', 'B-roomno'], ['9', 'B-roomno', 'B-roomno'], ['7', 'B-roomno', 'B-roomno'], [], ['浙', 'B-prov', 'B-prov'], ['江', 'I-prov', 'I-prov'], ['省', 'I-prov', 'I-prov'], ['嘉', 'B-city', 'B-city'], ['兴', 'I-city', 'I-city'], ['市', 'I-city', 'I-city']]\n",
      "0.8159865886717077\n",
      "Can't open perl script \"conlleval.pl\": No such file or directory\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [01:19<00:00,  3.52it/s]\n",
      "  0%|          | 0/94 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 Epoch Mean Loss 0.4930 Time 1.32 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:08<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['龙', 'B-town', 'B-town'], ['港', 'I-town', 'I-town'], ['镇', 'I-town', 'I-town'], ['泰', 'B-poi', 'B-poi'], ['和', 'I-poi', 'I-poi'], ['小', 'I-poi', 'I-poi'], ['区', 'I-poi', 'I-poi'], ['B', 'B-houseno', 'B-houseno'], ['懂', 'I-houseno', 'B-redundant'], ['1', 'B-roomno', 'B-roomno'], ['0', 'B-roomno', 'B-roomno'], ['9', 'B-roomno', 'B-roomno'], ['7', 'B-roomno', 'B-roomno'], [], ['浙', 'B-prov', 'B-prov'], ['江', 'I-prov', 'I-prov'], ['省', 'I-prov', 'I-prov'], ['嘉', 'B-city', 'B-city'], ['兴', 'I-city', 'I-city'], ['市', 'I-city', 'I-city']]\n",
      "0.8364424311599372\n",
      "Can't open perl script \"conlleval.pl\": No such file or directory\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [01:19<00:00,  3.51it/s]\n",
      "  0%|          | 0/94 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 Epoch Mean Loss 0.4327 Time 1.33 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:08<00:00, 10.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['龙', 'B-town', 'B-town'], ['港', 'I-town', 'I-town'], ['镇', 'I-town', 'I-town'], ['泰', 'B-poi', 'B-poi'], ['和', 'I-poi', 'I-poi'], ['小', 'I-poi', 'I-poi'], ['区', 'I-poi', 'I-poi'], ['B', 'B-houseno', 'B-houseno'], ['懂', 'I-houseno', 'I-subpoi'], ['1', 'B-roomno', 'B-roomno'], ['0', 'B-roomno', 'B-roomno'], ['9', 'B-roomno', 'B-roomno'], ['7', 'B-roomno', 'B-roomno'], [], ['浙', 'B-prov', 'B-prov'], ['江', 'I-prov', 'I-prov'], ['省', 'I-prov', 'I-prov'], ['嘉', 'B-city', 'B-city'], ['兴', 'I-city', 'I-city'], ['市', 'I-city', 'I-city']]\n",
      "0.8472856327578827\n",
      "Can't open perl script \"conlleval.pl\": No such file or directory\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [01:19<00:00,  3.51it/s]\n",
      "  0%|          | 0/94 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 Epoch Mean Loss 0.3947 Time 1.33 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:08<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['龙', 'B-town', 'B-town'], ['港', 'I-town', 'I-town'], ['镇', 'I-town', 'I-town'], ['泰', 'B-poi', 'B-poi'], ['和', 'I-poi', 'I-poi'], ['小', 'I-poi', 'I-poi'], ['区', 'I-poi', 'I-poi'], ['B', 'B-houseno', 'B-subpoi'], ['懂', 'I-houseno', 'I-subpoi'], ['1', 'B-roomno', 'B-roomno'], ['0', 'B-roomno', 'B-roomno'], ['9', 'B-roomno', 'B-roomno'], ['7', 'B-roomno', 'B-roomno'], [], ['浙', 'B-prov', 'B-prov'], ['江', 'I-prov', 'I-prov'], ['省', 'I-prov', 'I-prov'], ['嘉', 'B-city', 'B-city'], ['兴', 'I-city', 'I-city'], ['市', 'I-city', 'I-city']]\n",
      "0.8522970466543016\n",
      "Can't open perl script \"conlleval.pl\": No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_train_epochs):\n",
    "    # train\n",
    "    epoch_loss = None\n",
    "    epoch_step = 0\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for step, batch in enumerate(tqdm(train_data_loader)):\n",
    "        input_ids, attention_mask, label = (b.to(device) for b in batch[:-1])\n",
    "        loss = model(input_ids, attention_mask, label)\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step() \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        if epoch_loss is None:\n",
    "            epoch_loss = loss.item()\n",
    "        else:\n",
    "            epoch_loss = 0.98*epoch_loss + 0.02*loss.item()\n",
    "        epoch_step += 1\n",
    "    \n",
    "    used_time = (time.time() - start_time)/60\n",
    "    log('Epoch = %d Epoch Mean Loss %.4f Time %.2f min' % (epoch, epoch_loss, used_time))\n",
    "    result = eval()\n",
    "    with open('result.txt', 'w') as f:\n",
    "        for r in result:\n",
    "            f.write('\\t'.join(r) + '\\n')\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for r in result:\n",
    "        if not r: continue\n",
    "        y_true.append(label2id[r[1]])\n",
    "        y_pred.append(label2id[r[2]])\n",
    "    print(sklearn.metrics.f1_score(y_true, y_pred, average='micro'))\n",
    "    !perl conlleval.pl < result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T10:55:25.770938Z",
     "start_time": "2021-01-07T10:55:25.583151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "龙\tB-town\tB-town\r\n",
      "港\tI-town\tI-town\r\n",
      "镇\tI-town\tI-town\r\n",
      "泰\tB-poi\tB-poi\r\n",
      "和\tI-poi\tI-poi\r\n",
      "小\tI-poi\tI-poi\r\n",
      "区\tI-poi\tI-poi\r\n",
      "B\tB-houseno\tB-subpoi\r\n",
      "懂\tI-houseno\tI-subpoi\r\n",
      "1\tB-roomno\tB-roomno\r\n",
      "0\tB-roomno\tB-roomno\r\n",
      "9\tB-roomno\tB-roomno\r\n",
      "7\tB-roomno\tB-roomno\r\n",
      "\r\n",
      "浙\tB-prov\tB-prov\r\n"
     ]
    }
   ],
   "source": [
    "!head result.txt -n15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存和压缩模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T11:36:14.725963Z",
     "start_time": "2021-01-02T11:36:13.291624Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Neural_Chinese_Address_Parsing_BERT_state_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T11:36:59.892089Z",
     "start_time": "2021-01-02T11:36:32.209145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: Neural_Chinese_Address_Parsing_BERT_state_dict.pkl (deflated 7%)\n"
     ]
    }
   ],
   "source": [
    "!zip Neural_Chinese_Address_Parsing_BERT_state_dict.pkl.zip Neural_Chinese_Address_Parsing_BERT_state_dict.pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
