{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于文件 Tencent_AILab_ChineseEmbedding.txt 太大，需要先到 https://ai.tencent.com/ailab/nlp/zh/embedding.html 下载，并解压出该文件到同一目录下才可运行本代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-04T15:23:55.219573Z",
     "end_time": "2023-05-04T15:23:55.440756Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义两个list分别存放两个板块的帖子数据\n",
    "import jieba\n",
    "academy_titles = []\n",
    "job_titles = []\n",
    "with open('academy_titles.txt', encoding='utf8') as f:\n",
    "    for l in f:  # 按行读取文件\n",
    "        academy_titles.append(list(jieba.cut(l.strip( ))))  # strip 方法用于去掉行尾空格\n",
    "with open('job_titles.txt', encoding='utf8') as f:\n",
    "    for l in f:  # 按行读取文件\n",
    "        job_titles.append(list(jieba.cut(l.strip( ))))  # strip 方法用于去掉行尾空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-04T15:23:55.441561Z",
     "end_time": "2023-05-04T15:23:55.443764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['出售', '人大', '新闻', '学院', '2015', '年', '考研', '权威', '资料']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "academy_titles[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-04T15:23:55.452752Z",
     "end_time": "2023-05-04T15:23:55.454821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4059\n"
     ]
    }
   ],
   "source": [
    "word_set = set()\n",
    "for title in academy_titles:\n",
    "    for word in title:\n",
    "        word_set.add(word.lower())\n",
    "for title in job_titles:\n",
    "    for word in title:\n",
    "        word_set.add(word.lower())\n",
    "print(len(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:35:55.130880Z",
     "start_time": "2020-09-08T13:32:31.339004Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Tencent_AILab_ChineseEmbedding.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[0;32m----> 2\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTencent_AILab_ChineseEmbedding.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m word2v \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      4\u001B[0m wl \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m     )\n\u001B[0;32m--> 284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Tencent_AILab_ChineseEmbedding.txt'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "f = open('Tencent_AILab_ChineseEmbedding.txt', encoding='utf8')\n",
    "word2v = {}\n",
    "wl = []\n",
    "for l in tqdm(f):\n",
    "    l = l.strip().split(' ')\n",
    "    wl.append(l[0])\n",
    "    if l[0] in word_set:\n",
    "        word2v[l[0]] = list(map(float, l[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:38:39.622859Z",
     "start_time": "2020-09-08T13:38:39.609868Z"
    }
   },
   "outputs": [],
   "source": [
    "len(word2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:52:03.518770Z",
     "start_time": "2020-09-08T13:52:03.250626Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('word2v', 'r') as f:\n",
    "    word2v = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:52:06.290577Z",
     "start_time": "2020-09-08T13:52:06.272587Z"
    }
   },
   "outputs": [],
   "source": [
    "len(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:38:47.219700Z",
     "start_time": "2020-09-08T13:38:47.208704Z"
    }
   },
   "outputs": [],
   "source": [
    "4085-3924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:41:41.922297Z",
     "start_time": "2020-09-08T13:41:41.912303Z"
    }
   },
   "outputs": [],
   "source": [
    "dx = []\n",
    "for wd in word_set:\n",
    "    if wd.lower() not in word2v:\n",
    "        dx.append(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:41:42.600969Z",
     "start_time": "2020-09-08T13:41:42.581980Z"
    }
   },
   "outputs": [],
   "source": [
    "len(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:30:46.835863Z",
     "start_time": "2020-09-08T13:30:46.829865Z"
    }
   },
   "outputs": [],
   "source": [
    "len(wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:31:32.937113Z",
     "start_time": "2020-09-08T13:31:32.925121Z"
    }
   },
   "outputs": [],
   "source": [
    "'NLP'.lower() in wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:41:45.650372Z",
     "start_time": "2020-09-08T13:41:45.638380Z"
    }
   },
   "outputs": [],
   "source": [
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:52:16.867804Z",
     "start_time": "2020-09-08T13:52:10.369917Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def title_to_tensor(title):\n",
    "    words_vectors = []\n",
    "    for word in title:\n",
    "        if word in word2v:\n",
    "            words_vectors.append(word2v[word])\n",
    "    tensor = torch.tensor(words_vectors, dtype=torch.float)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:52:16.883791Z",
     "start_time": "2020-09-08T13:52:16.871798Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(embedding_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(embedding_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_tensor, hidden):\n",
    "        word_vector = input_tensor\n",
    "        combined = torch.cat((word_vector, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "embedding_size = 200\n",
    "n_hidden = 128\n",
    "n_categories = 2\n",
    "rnn = RNN(embedding_size, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:47:31.538306Z",
     "start_time": "2020-09-08T13:47:31.524312Z"
    }
   },
   "outputs": [],
   "source": [
    "input_tensor = title_to_tensor(academy_titles[0])\n",
    "print('input_tensor:\\n', input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:47:32.126867Z",
     "start_time": "2020-09-08T13:47:32.116875Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden = rnn.initHidden()\n",
    "output, hidden = rnn(input_tensor[0].unsqueeze(dim=0), hidden)\n",
    "print('output:\\n', output)\n",
    "print('hidden:\\n', hidden)\n",
    "print('size of hidden:\\n', hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:52:19.129024Z",
     "start_time": "2020-09-08T13:52:19.122030Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_rnn(rnn, input_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    for i in range(input_tensor.size()[0]):\n",
    "        output, hidden = rnn(input_tensor[i].unsqueeze(dim=0), hidden)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:52:19.641152Z",
     "start_time": "2020-09-08T13:52:19.633158Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data = []\n",
    "categories = [\"考研考博\", \"招聘信息\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:52:20.474833Z",
     "start_time": "2020-09-08T13:52:20.080282Z"
    }
   },
   "outputs": [],
   "source": [
    "for l in academy_titles:\n",
    "    all_data.append((title_to_tensor(l), torch.tensor([0], dtype=torch.long)))\n",
    "for l in job_titles:\n",
    "    all_data.append((title_to_tensor(l), torch.tensor([1], dtype=torch.long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:52:21.714629Z",
     "start_time": "2020-09-08T13:52:21.699636Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(all_data)\n",
    "data_len = len(all_data)\n",
    "split_ratio = 0.7\n",
    "train_data = all_data[:int(data_len*split_ratio)]\n",
    "test_data = all_data[int(data_len*split_ratio):]\n",
    "print(\"Train data size: \", len(train_data))\n",
    "print(\"Test data size: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:52:23.369901Z",
     "start_time": "2020-09-08T13:52:23.363903Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(rnn, criterion, input_tensor, category_tensor):\n",
    "    rnn.zero_grad()\n",
    "    output = run_rnn(rnn, input_tensor)\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # 根据梯度更新模型的参数\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:52:23.770701Z",
     "start_time": "2020-09-08T13:52:23.760697Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(rnn, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.initHidden()\n",
    "        output = run_rnn(rnn, input_tensor)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:52:36.755308Z",
     "start_time": "2020-09-08T13:52:25.585350Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "epoch = 1\n",
    "embedding_size = 200\n",
    "n_hidden = 10\n",
    "n_categories = 2\n",
    "learning_rate = 0.005\n",
    "rnn = RNN(embedding_size, n_hidden, n_categories)\n",
    "criterion = nn.NLLLoss()\n",
    "loss_sum = 0\n",
    "all_losses = []\n",
    "plot_every = 100\n",
    "for e in range(epoch):\n",
    "    for ind, (title_tensor, label) in enumerate(tqdm(train_data)):\n",
    "        if len(title_tensor) == 0:\n",
    "            continue\n",
    "        output, loss = train(rnn, criterion, title_tensor, label)\n",
    "        loss_sum += loss\n",
    "        if ind % plot_every == 0:\n",
    "            all_losses.append(loss_sum / plot_every)\n",
    "            loss_sum = 0\n",
    "    c = 0\n",
    "    for title, category in tqdm(test_data):\n",
    "        if len(title) == 0:\n",
    "            continue\n",
    "        output = evaluate(rnn, title)\n",
    "        topn, topi = output.topk(1)\n",
    "        if topi.item() == category[0].item():\n",
    "            c += 1\n",
    "    print('accuracy', c / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:34:02.972596Z",
     "start_time": "2020-08-20T13:33:58.836172Z"
    }
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "l1 = []\n",
    "l2 = []\n",
    "for title, category in tqdm(test_data):\n",
    "    output = evaluate(rnn, title)\n",
    "    topn, topi = output.topk(1)\n",
    "    l1.append(topi.item())\n",
    "    l2.append(category[0].item())\n",
    "    if topi.item() == category[0].item():\n",
    "        c += 1\n",
    "print('accuracy', c / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:34:12.363857Z",
     "start_time": "2020-08-20T13:34:12.348897Z"
    }
   },
   "outputs": [],
   "source": [
    "print(l1[:40])\n",
    "print(l2[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:34:24.323156Z",
     "start_time": "2020-08-20T13:34:24.315177Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:34:27.957712Z",
     "start_time": "2020-08-20T13:34:27.946694Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:28:46.089108Z",
     "start_time": "2020-08-20T13:28:42.250574Z"
    }
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "for title, category in tqdm(test_data):\n",
    "    output = evaluate(rnn, title)\n",
    "    topn, topi = output.topk(1)\n",
    "    if topi.item() == category[0].item():\n",
    "        c += 1\n",
    "print('accuracy', c / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:01:52.495244Z",
     "start_time": "2020-08-20T13:01:52.303270Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.ylabel('Average Loss')\n",
    "plt.plot(all_losses[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:16:40.170444Z",
     "start_time": "2020-08-20T13:16:40.064366Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(rnn, 'rnn_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:24:52.707246Z",
     "start_time": "2020-08-20T13:24:52.698271Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_category(rnn, title):\n",
    "    input_tensor = title_to_tensor(title)\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.initHidden()\n",
    "        output = run_rnn(rnn, input_tensor)\n",
    "        topv, topi = output.topk(1)\n",
    "        return categories[topi.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:30:17.778502Z",
     "start_time": "2020-08-20T13:30:17.773516Z"
    }
   },
   "outputs": [],
   "source": [
    "input_tensor = title_to_tensor(\"北大实验室招硕博连读保研学生\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:37:26.165119Z",
     "start_time": "2020-08-20T13:37:26.148164Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_category(title):\n",
    "    title = title_to_tensor(title)\n",
    "    output = evaluate(rnn, title)\n",
    "    topn, topi = output.topk(1)\n",
    "    return categories[topi.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:32:53.017140Z",
     "start_time": "2020-08-20T13:32:53.004177Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"【校招】今日头条后端开发工程师\", get_category(rnn, \"【校招】今日头条后端开发工程师\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:33:08.592301Z",
     "start_time": "2020-08-20T13:33:08.580281Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"毕业找房子\", get_category(rnn, \"毕业找房子\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:28:25.755093Z",
     "start_time": "2020-08-20T13:28:25.739137Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"学校附近的公寓\", get_category(rnn, \"学校附近的公寓\"))\n",
    "print(\"学校附近的公寓\", get_category(rnn, \"学校附近的公寓\"))\n",
    "print(\"考博经验帖\", get_category(rnn, \"考博经验帖\"))\n",
    "print(\"2021年秋季出国交流\", get_category(rnn, \"2021年秋季出国交流\"))\n",
    "print(\"考研学校选择，纠结，求师哥师姐指导\", get_category(rnn, \"考研学校选择，纠结，求师哥师姐指导\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
